{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Proposal Learning",
   "id": "9b0e04ba1a8dec92"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Before running this notebook to run the experiments you must generate the dataset.\n",
    " \n",
    "This is done by running generate_data.py from the directory that contains both it and this notebook. Optionally the dimensionality of the state and observations can be controlled by the command line arguments --dx and --dy respectively, then running the model will require setting dx and dy in the options cell of this notebook. By default dx = 25, dy = 1 mirroring the experiment setting in the paper. The batch size can be specified by the argument --batch_size, default: 128."
   ],
   "id": "ffdb5fb59061b040"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pydpf\n",
    "import lg_model\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from math import sqrt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dx = 25\n",
    "dy = 1\n",
    "n_repeats = 5\n",
    "batch_size = 32\n",
    "data_path = pathlib.Path('.').parent.absolute().joinpath(f'data/dx={dx}-dy={dy}.csv')\n",
    "result_path = pathlib.Path('.').parent.absolute().joinpath('results/proposal_learning_results.csv')\n",
    "generate_data = False\n",
    "experiment_list = ['Bootstrap', 'Optimal', 'DPF', 'Soft', 'Stop-Gradient', 'Marginal Stop-Gradient', 'Optimal Transport', 'Kernel']\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cuda_gen = torch.Generator(device=device).manual_seed(0)\n",
    "cpu_gen = torch.Generator().manual_seed(0)"
   ],
   "id": "8f836606d041aa09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def make_model_componets(dx, dy, generator, optimal_prop = True):\n",
    "    dynamic_model = lg_model.GaussianDynamic(dx, generator)\n",
    "    observation_model = lg_model.GaussianObservation(dx, dy, generator)\n",
    "    prior_model = lg_model.GaussianPrior(dx, generator)\n",
    "    if optimal_prop:\n",
    "        proposal_model = lg_model.GaussianOptimalProposal(dx, dy, generator)\n",
    "    else:\n",
    "        proposal_model = lg_model.GaussianLearnedProposal(dx, dy, generator)\n",
    "    return prior_model, dynamic_model, observation_model, proposal_model"
   ],
   "id": "f6d4c6b36aa55a8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_DPF(DPF_type, SSM, dim):\n",
    "    if DPF_type == 'DPF':\n",
    "        return pydpf.DPF(SSM=SSM, resampling_generator=cuda_gen)\n",
    "    if DPF_type == 'Soft':\n",
    "        return pydpf.SoftDPF(SSM=SSM, resampling_generator=cuda_gen)\n",
    "    if DPF_type == 'Stop-Gradient':\n",
    "        return pydpf.StopGradientDPF(SSM=SSM, resampling_generator=cuda_gen)\n",
    "    if DPF_type == 'Marginal Stop-Gradient':\n",
    "        return pydpf.MarginalStopGradientDPF(SSM=SSM, resampling_generator=cuda_gen)\n",
    "    if DPF_type == 'Optimal Transport':\n",
    "        return pydpf.OptimalTransportDPF(SSM=SSM, regularisation=0.5)\n",
    "    if DPF_type == 'Kernel':\n",
    "        kernel = pydpf.KernelMixture(pydpf.MultivariateGaussian(torch.zeros(25, device=device),torch.nn.Parameter(torch.eye(25, device=device)*0.1), diagonal_cov=True, generator=cuda_gen), generator=cuda_gen)\n",
    "        return pydpf.KernelDPF(SSM=SSM, kernel=kernel)\n",
    "    raise ValueError('DPF_type should be one of the allowed options')"
   ],
   "id": "7cd900020f0d005a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def training_loop(dpf, epochs, train_loader):\n",
    "    ELBO_fun = pydpf.ElBO_Loss()\n",
    "    if experiment == 'Kernel':\n",
    "        opt = torch.optim.SGD([{'params': [dpf.SSM.proposal_model.x_weight], 'lr' : 0.01}, {'params': [dpf.SSM.proposal_model.y_weight, proposal_model.dist.cholesky_covariance], 'lr' : 0.05}, { 'params': dpf.resampler.parameters(), 'lr': 0.001}], lr=.5, momentum=0.9, nesterov=True)\n",
    "    elif experiment == 'Optimal Transport':\n",
    "        opt = torch.optim.SGD([{'params': [dpf.SSM.proposal_model.x_weight], 'lr' : 0.01}, {'params': [dpf.SSM.proposal_model.y_weight, proposal_model.dist.cholesky_covariance], 'lr' : 0.05}], lr=.5, momentum=0.9, nesterov=True)\n",
    "    else:\n",
    "        opt = torch.optim.SGD([{'params': [dpf.SSM.proposal_model.x_weight], 'lr' : 0.1}, {'params': [dpf.SSM.proposal_model.y_weight, proposal_model.dist.cholesky_covariance], 'lr' : 0.5}], lr=.5, momentum=0.9, nesterov=True)\n",
    "    opt_scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.99)\n",
    "    best_validation_loss = torch.inf\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        dpf.train()\n",
    "        total_size = 0\n",
    "        train_loss = []\n",
    "        for state, observation in train_loader:\n",
    "            dpf.update()\n",
    "            opt.zero_grad()\n",
    "            ELBO = dpf(100, 100, ELBO_fun, observation=observation)\n",
    "            loss = torch.mean(ELBO)\n",
    "            loss.backward()\n",
    "            train_loss.append(loss.item()*state.size(1))\n",
    "            opt.step()\n",
    "            total_size += state.size(1)\n",
    "            opt_scheduler.step()\n",
    "        train_loss = np.sum(np.array(train_loss)) / total_size\n",
    "        \n",
    "        dpf.eval()\n",
    "        dpf.update()\n",
    "        total_size = 0\n",
    "        validation_loss = []\n",
    "        with torch.inference_mode():\n",
    "            for state, observation in train_loader:\n",
    "                ELBO = dpf(100, 100, ELBO_fun, observation=observation)\n",
    "                loss = torch.mean(ELBO)\n",
    "                validation_loss.append(loss.item()*state.size(1))\n",
    "                total_size += state.size(1)\n",
    "            validation_loss = np.sum(np.array(validation_loss)) / total_size\n",
    "    \n",
    "        if validation_loss < best_validation_loss:\n",
    "            best_validation_loss = validation_loss\n",
    "            best_dict = deepcopy(dpf.state_dict())\n",
    "        dpf.load_state_dict(best_dict)"
   ],
   "id": "9f53e4574481d6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def fractional_diff_exp(a, b):\n",
    "    frac = b-a\n",
    "    return torch.abs(1 - torch.exp(frac))\n",
    "\n",
    "def test_dpf(dpf, test_loader, KalmanFilter):\n",
    "    aggregation_fun = {'ELBO': pydpf.ElBO_Loss(), 'Filtering Mean': pydpf.FilteringMean(), 'Likelihood_factors': pydpf.LogLikelihoodFactors()}\n",
    "    test_ELBO = []\n",
    "    epsilon_x = []\n",
    "    epsilon_l = []\n",
    "    dpf.update()\n",
    "    total_size = 0\n",
    "    with torch.inference_mode():\n",
    "        for state, observation in test_loader:\n",
    "            outputs = dpf(n_particles = 100, time_extent=1000, aggregation_function=aggregation_fun, observation=observation)\n",
    "            test_ELBO.append(outputs['ELBO'].sum().item() * state.size(1))\n",
    "            kalman_state, kalman_cov, kalman_likelihood = KalmanFilter(observation=observation, time_extent=1000)\n",
    "            epsilon_x.append(torch.sum((outputs['Filtering Mean'] - kalman_state)**2, dim=-1).mean().item() * state.size(1))\n",
    "            log_abs_likelihood_error = fractional_diff_exp(kalman_likelihood, outputs['Likelihood_factors'].squeeze()).mean()\n",
    "            epsilon_l.append(log_abs_likelihood_error.item() * state.size(1))\n",
    "            total_size += state.size(1)\n",
    "    return -sum(test_ELBO)/total_size, sum(epsilon_x)/total_size, sum(epsilon_l)/total_size\n",
    "    "
   ],
   "id": "5aa3ab10abfbbe6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def max_wass_dist(x_weight, y_weight, prop_cov):\n",
    "    optimal_x_weight = torch.ones(dx, device=device)\n",
    "    optimal_x_weight[:dy] = .5\n",
    "    optimal_cov = torch.ones(dx, device=device)\n",
    "    for i in range(dy):\n",
    "        optimal_cov[i] = .5\n",
    "    a = x_weight - optimal_x_weight\n",
    "    b = y_weight - .5\n",
    "    if torch.all(a == 0):\n",
    "        mean_div = torch.zeros(dy, device=device)\n",
    "    else:\n",
    "        mean_div = a**2/torch.sum(a**2)\n",
    "    if torch.all(b == 0):\n",
    "        y_mean_div_contr = 0\n",
    "    else:\n",
    "        y_mean_div_contr = b**2/torch.sum(b**2)\n",
    "    mean_div[:dy] += y_mean_div_contr\n",
    "    mean_div = torch.sum(mean_div**2)\n",
    "    cov_div = torch.sum((optimal_cov + prop_cov - 2*torch.sqrt(optimal_cov*prop_cov)))\n",
    "    return mean_div + cov_div\n",
    "    "
   ],
   "id": "a14d1c71210a3be3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def chain(*its):\n",
    "    it_list = []\n",
    "    for it in its:\n",
    "        it_list += list(it)\n",
    "    return it_list\n",
    "        \n",
    "def rotate_range(c_repeat, rel_start, rel_end, repeats, total_elements):\n",
    "    range_rotation_amount = (total_elements // repeats)*c_repeat\n",
    "    start = (rel_start + range_rotation_amount) % total_elements\n",
    "    end = (rel_end + range_rotation_amount) % total_elements\n",
    "    if end == 0:\n",
    "        return range(start, total_elements)\n",
    "    if start > end:\n",
    "        return chain(range(start, total_elements), range(0, end))\n",
    "    return range(start, end)\n",
    "    "
   ],
   "id": "f65eb42d05b7c77a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset = pydpf.StateSpaceDataset(data_path=data_path, \n",
    "                        series_id_column='series_id', \n",
    "                        state_prefix='state', \n",
    "                        observation_prefix='observation', \n",
    "                        device=device)\n",
    "if dy > dx:\n",
    "    raise ValueError('The dimension of the observations cannot be more than the dimension of the states.')\n",
    "\n",
    "for experiment in experiment_list:\n",
    "    print(f\"Running {experiment}\")\n",
    "    mean_wass_dist = torch.tensor(0., device=device)\n",
    "    mean_epsilon_l = 0\n",
    "    mean_epsilon_x = 0\n",
    "    mean_ELBO = 0\n",
    "    for repeat in range(n_repeats):\n",
    "        print(f\"Repeat {repeat+1} of {n_repeats}:\")\n",
    "        cpu_gen = torch.Generator().manual_seed(10*repeat)\n",
    "        cuda_gen = torch.Generator(device=device).manual_seed(10*repeat)\n",
    "        #Cyclically permute the indices of the dataset, so that across repeats we cover whole dataset during testing. \n",
    "        train_set = torch.utils.data.Subset(dataset, rotate_range(repeat, 0, 1000, n_repeats, 2000))\n",
    "        validation_set = torch.utils.data.Subset(dataset, rotate_range(repeat, 1000, 1500, n_repeats, 2000))\n",
    "        test_set = torch.utils.data.Subset(dataset, rotate_range(repeat, 1500, 2000, n_repeats, 2000))\n",
    "        prior_model, dynamic_model , observation_model, proposal_model = make_model_componets(dx, dy, cuda_gen, experiment == 'Optimal')\n",
    "        if experiment == 'Bootstrap':\n",
    "            SSM = pydpf.FilteringModel(prior_model=prior_model, dynamic_model=dynamic_model, observation_model=observation_model)\n",
    "            dpf = get_DPF('DPF', SSM, dx)\n",
    "            mean_wass_dist += max_wass_dist(torch.ones(dx, device=device), torch.zeros(dy, device=device), torch.ones(dx, device=device))\n",
    "                \n",
    "        elif experiment == 'Optimal':\n",
    "            SSM = pydpf.FilteringModel(prior_model=prior_model, dynamic_model=dynamic_model, observation_model=observation_model, proposal_model=proposal_model)\n",
    "            dpf = get_DPF('DPF', SSM, dx)\n",
    "        else:\n",
    "            trained_model = pydpf.FilteringModel(prior_model=prior_model, dynamic_model=dynamic_model, observation_model=observation_model, proposal_model=proposal_model)\n",
    "            dpf = get_DPF(experiment, trained_model, dx)\n",
    "            train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, generator=cpu_gen, collate_fn=dataset.collate)\n",
    "            validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, shuffle=False, generator=cpu_gen, collate_fn=dataset.collate)\n",
    "            training_loop(dpf, 20, train_loader)\n",
    "            cholesky_prop_cov = torch.diag(proposal_model.dist.cholesky_covariance)\n",
    "            prop_cov = cholesky_prop_cov**2\n",
    "            mean_wass_dist += max_wass_dist(proposal_model.x_weight, proposal_model.y_weight, prop_cov)\n",
    "            \n",
    "\n",
    "        test_loader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, shuffle=False, generator=cpu_gen, collate_fn=dataset.collate)\n",
    "        kalman_filter = pydpf.KalmanFilter(prior_model=prior_model, dynamic_model=dynamic_model, observation_model=observation_model)\n",
    "        ELBO, e_x, e_l = test_dpf(dpf, test_loader, kalman_filter)\n",
    "        mean_ELBO += ELBO\n",
    "        mean_epsilon_l += e_l\n",
    "        mean_epsilon_x += e_x\n",
    "    mean_wass_dist = sqrt(mean_wass_dist.item() / n_repeats)\n",
    "    mean_ELBO = mean_ELBO / n_repeats\n",
    "    mean_epsilon_x = mean_epsilon_x / n_repeats\n",
    "    mean_epsilon_l = mean_epsilon_l / n_repeats\n",
    "    results_df = pd.read_csv(result_path, index_col=0)\n",
    "    row = np.array([mean_epsilon_x, mean_epsilon_l, mean_wass_dist, mean_ELBO])\n",
    "    results_df.loc[experiment] = row\n",
    "    results_df.to_csv(result_path)\n",
    "    print(results_df)"
   ],
   "id": "e147552246f81716",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
