{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Unsupervised learning of a single parameter",
   "id": "44e1f61f4aeebea4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1b25257410b38fdf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Before running this notebook: ensure that the steps outlined in sv_setup_instructions.txt are complete.",
   "id": "161065c1f9a10e7a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pydpf\n",
    "import numpy as np\n",
    "import torch\n",
    "import pathlib\n",
    "import sv_model\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from sv_training_loop import train\n",
    "import pandas as pd\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Experiment Settings",
   "id": "32f2932c7bddd375"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_repeats = 10\n",
    "training_epochs = 10\n",
    "batch_size_test = 128\n",
    "batch_size_train = 32\n",
    "#Setting the true parameters here will only affect the testing of the ability to learn alpha and not the standard deviation of the gradients which is locked with alpha = 0.91.\n",
    "true_alpha = 0.91\n",
    "true_beta = 0.5\n",
    "true_sigma = 1.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "result_path = pathlib.Path('.').parent.absolute().joinpath('results/single_parameter_results.csv')\n",
    "data_path = pathlib.Path('.').parent.absolute().joinpath('data/')\n",
    "temp_data_path =  data_path / \"temp.csv\"\n",
    "experiments = ['DPF', 'Soft', 'Stop-Gradient', 'Marginal Stop-Gradient', 'Optimal Transport', 'Kernel']\n",
    "if not (data_path / \"test_trajectory.csv\").exists():\n",
    "    raise FileNotFoundError(\"No file test_trajectory.csv found in the data directory, please download it from out github before running this notebook.\")"
   ],
   "id": "4ccc64ac86d7b6ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_DPF(DPF_type, SSM, rng):\n",
    "    if DPF_type == 'DPF':\n",
    "        return pydpf.DPF(SSM=SSM, resampling_generator=rng)\n",
    "    if DPF_type == 'Soft':\n",
    "        return pydpf.SoftDPF(SSM=SSM, resampling_generator=rng)\n",
    "    if DPF_type == 'Stop-Gradient':\n",
    "        return pydpf.StopGradientDPF(SSM=SSM, resampling_generator=rng)\n",
    "    if DPF_type == 'Marginal Stop-Gradient':\n",
    "        return pydpf.MarginalStopGradientDPF(SSM=SSM, resampling_generator=rng)\n",
    "    if DPF_type == 'Optimal Transport':\n",
    "        return pydpf.OptimalTransportDPF(SSM=SSM, regularisation=0.5, transport_gradient_clip=1.)\n",
    "    if DPF_type == 'Kernel':\n",
    "        kernel = pydpf.KernelMixture(pydpf.MultivariateGaussian(torch.zeros(1, device=device),torch.nn.Parameter(torch.eye(1, device=device)*0.1), generator=rng), generator=rng)\n",
    "        return pydpf.KernelDPF(SSM=SSM, kernel=kernel)\n",
    "    raise ValueError('DPF_type should be one of the allowed options')"
   ],
   "id": "9cf2a81ff69115d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Timing and testing the variance of the gradients on a single given trajectory",
   "id": "15548570bde85741"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def test_gradients(experiment):\n",
    "    experiment_cuda_rng = torch.Generator(device).manual_seed(0)\n",
    "    aggregation_function_dict = {'ELBO': pydpf.LogLikelihoodFactors()}\n",
    "    test_dataset = pydpf.StateSpaceDataset(data_path=data_path / \"test_trajectory.csv\", state_prefix='state', device='cuda')\n",
    "    gradients = []\n",
    "    size = 0\n",
    "    alpha_p = torch.nn.Parameter(torch.tensor([[0.93]], dtype = torch.float32, device=device))\n",
    "    SSM = sv_model.make_SSM(torch.tensor([[1.]], device=device), alpha_p, torch.tensor([0.5], device=device), device)\n",
    "    DPF = get_DPF(experiment, SSM, experiment_cuda_rng)\n",
    "    forward_time = []\n",
    "    backward_time = []\n",
    "    state = test_dataset.state[:,0:1].expand((101, batch_size_test, 1)).contiguous()\n",
    "    observation = test_dataset.observation[:,0:1].expand((101, batch_size_test, 1)).contiguous()\n",
    "    for i in tqdm(range(2560//batch_size_test)):\n",
    "        DPF.update()\n",
    "        size += state.size(1)\n",
    "        torch.cuda.synchronize()\n",
    "        start = time()\n",
    "        outputs = DPF(observation=observation, n_particles=100, ground_truth=state, aggregation_function=aggregation_function_dict, time_extent=100)\n",
    "        ls = torch.mean(outputs['ELBO'], dim=0)\n",
    "        loss = ls.mean()\n",
    "        torch.cuda.synchronize()\n",
    "        forward_time.append((time() - start))\n",
    "        alpha_p.grad = None\n",
    "        for i in range(len(ls)):\n",
    "            ls[i].backward(retain_graph=True)\n",
    "            gradients.append(alpha_p.grad.item())\n",
    "            alpha_p.grad = None\n",
    "        #Free the stored tensors\n",
    "        torch.cuda.synchronize()\n",
    "        start = time()\n",
    "        loss.backward()\n",
    "        torch.cuda.synchronize()\n",
    "        backward_time.append((time() - start))\n",
    "    return forward_time, backward_time, gradients"
   ],
   "id": "aa57f4668382c639",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def test_learning_alpha(experiment):\n",
    "    alphas = np.empty(n_repeats)\n",
    "    for n in range(n_repeats):\n",
    "        experiment_cuda_rng = torch.Generator(device).manual_seed(n*10)\n",
    "        experiment_cpu_rng = torch.Generator().manual_seed(n*10)\n",
    "        generation_rng = torch.Generator(device).manual_seed(n*10)\n",
    "        true_SSM = sv_model.make_SSM(torch.tensor([[true_sigma]], device=device), torch.tensor([[true_alpha]], device=device), torch.tensor([true_beta], device=device), device, generation_rng)\n",
    "        pydpf.simulate_and_save(temp_data_path, SSM=true_SSM, time_extent=1000, n_trajectories=500, batch_size=100, device=device, bypass_ask=True)\n",
    "        alpha = torch.nn.Parameter(torch.rand((1,1), device=device, generator=experiment_cuda_rng), requires_grad=True)\n",
    "        SSM = sv_model.make_SSM(torch.tensor([[1.]], device=device), alpha, torch.tensor([0.5], device=device), device, generation_rng)\n",
    "        dpf = get_DPF(experiment, SSM, experiment_cuda_rng)\n",
    "        if experiment == 'Kernel':\n",
    "            opt = torch.optim.SGD([{'params':[alpha], 'lr':0.05}, {'params':dpf.resampler.mixture.parameters(), 'lr':0.01}])\n",
    "        else:\n",
    "            opt = torch.optim.SGD([{'params':[alpha], 'lr':0.05}])\n",
    "        opt_schedule = torch.optim.lr_scheduler.ExponentialLR(opt, 0.95)\n",
    "        dataset = pydpf.StateSpaceDataset(temp_data_path, state_prefix='state', device=device)\n",
    "        _, ELBO = train(dpf, opt, dataset, training_epochs, (100, 100, 100), (batch_size_train, batch_size_test, batch_size_test), (0.5, 0.25, 0.25), 1., experiment_cpu_rng, target='ELBO', time_extent=100, lr_scheduler=opt_schedule)\n",
    "        alphas[n] = alpha\n",
    "        os.remove(temp_data_path)\n",
    "    return alphas\n"
   ],
   "id": "f770de140baeca54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for experiment in experiments:\n",
    "    print(f\"Testing {experiment}\")\n",
    "    results = pd.read_csv(result_path, index_col=0)\n",
    "    ft, bt, grads = test_gradients(experiment)\n",
    "    alpha_list = test_learning_alpha(experiment)\n",
    "    #Ignore first and last times because the last batch will have a different size to the rest, and cuda is often slower on the first iteration\n",
    "    row = np.array([sum(ft[1:-1])/(len(ft)-2), sum(bt[1:-1])/(len(bt)-2), np.sqrt(np.var(grads)), np.mean(np.abs(alpha_list - 0.91))])\n",
    "    results.loc[experiment] = row\n",
    "    print(results)\n",
    "    results.to_csv(result_path)"
   ],
   "id": "62b835a506c49f6a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
